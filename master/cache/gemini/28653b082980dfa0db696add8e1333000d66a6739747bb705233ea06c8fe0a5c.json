{
  "text": "Methods\nConcept Fidelity\nDecoupling Ability\nVideo Quality\nCLIP-T\u2191\nCLIP-I\u2191\nCLIP-T\u2191\nCLIP-I\u2191\nDINO-I\u2191\nMotion Smoothness\u2191\nDynamic Degree\u2191\nAesthetic Quality\u2191\nImaging Quality\u2191\nMerge Textual and Visual Embedd...",
  "prompt": "You are a research paper analyzer. Analyze the research methodology in this paper. If the methodology section is not clearly defined, look for methodological elements throughout the text.\n\nReturn your...",
  "response": "{\n    \"research_type\": \"empirical\",\n    \"methods_used\": [\n        \"Text-to-video generation\",\n        \"Diffusion models\",\n        \"Multi-concept image customization\",\n        \"Multi-concept video customization\",\n        \"Data collection pipeline\",\n        \"Q-Former network\",\n        \"Decouple Attention Module (DAM)\",\n        \"Multi-Concept Injector (MC-Injector)\"\n    ],\n    \"data_collection\": \"The data collection pipeline involves the following steps:\\n\\n1. **Video pool creation:** We start by creating a pool of videos from various sources on the Internet. This pool includes videos with multiple concepts, such as multiple persons, persons with livings, persons with stuffs, multiple livings, livings with stuffs, and persons with both livings and stuffs.\\n\\n2. **Video sampling:** From the video pool, we randomly sample 10% of the frames for each video.\\n\\n3. **Noun extraction:** For each sampled frame, we extract nouns using the SpaCy library. These nouns serve as the textual input for text-guided detection and segmentation algorithms.\\n\\n4. **Unsuitable video elimination:** We eliminate videos that are clearly unsuitable for video customization based on the following criteria:\\n    - Videos with low-quality frames\\n    - Videos with scene transitions\\n    - Videos with low dynamics\\n    - Videos with low light contrast\\n\\n5. **Fine-grained identity information extraction:** For each video that passes the unsuitable video elimination step, we extract fine-grained identity information using the following steps:\\n    - **Entity box extraction:** We use text-guided Grounding-DINO to extract entity boxes for each concept in the video.\\n    - **Box filtering:** We apply Non-Maximum Suppression (NMS) to filter out duplicate boxes and remove boxes that are either too large or too small.\\n    - **Box classification:** We classify each box using CLIP to eliminate any box if the label classified by CLIP is inconsistent with the original one.\\n    - **Face detection:** We use FaceAnalysis to detect all regions belonging to the person class, retaining only those that contain face regions.\\n\\n6. **Accurate visual and textual labels extraction:** To accurately extract the region and text label of each identity, we use LISA, an MLLM-based segmentor, input by both text prompts and images with strong visual reasoning capabilities, to extract entity masks. We then derive box regions from these masks and remove any misclassified ones through CLIP classification.\",\n    \"analysis_techniques\": [\n        \"CLIP-I\",\n        \"CLIP-T\",\n        \"DINO-I\",\n        \"Motion smoothness\",\n        \"Dynamic degree\",\n        \"Aesthetic quality\",\n        \"Imaging quality\"\n    ]\n}"
}