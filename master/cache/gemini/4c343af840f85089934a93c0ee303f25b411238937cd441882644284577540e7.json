{
  "text": "These approaches utilize no pixel-level human annotations and only image-\nlevel captions from noisy internet-scale datasets (same data used to train CLIP [42])\nOur proposed Seg-TTO achieves state-of-t...",
  "prompt": "You are a research paper analyzer. Analyze this research paper and extract key information. If you can't find certain information, focus on what you can find in the text.\n\nReturn your analysis in this...",
  "response": "{\n    \"main_topic\": \"Open vocabulary semantic segmentation (OVSS)\",\n    \"objective\": \"To develop a framework for zero-shot, open-vocabulary semantic segmentation (OVSS) that excels in specialized domain tasks.\",\n    \"methodology\": \"The proposed framework, Seg-TTO, utilizes a novel self-supervised objective to align model parameters with input images at test time. In the textual modality, multiple embeddings are learned for each category to capture diverse concepts within an image. In the visual modality, pixel-level losses are calculated followed by embedding aggregation operations specific to preserving spatial structure.\",\n    \"key_findings\": \"Seg-TTO demonstrates clear performance improvements across 22 challenging OVSS tasks covering a range of specialized domains, establishing new state-of-the-art.\"\n}"
}